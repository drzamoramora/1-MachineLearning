{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingeniería de metadatos\n",
    "\n",
    "La ingeniería de metadatos (“feature engineering”) es una de las opciones para mejorar modelos de machine learning. Podemos crear nuevas variables (“nuevos features”), o bien transformarlas, adaptarlas o normalizarlas para que los modelos puedan realizar un uso más depurado de las mismas. \n",
    "\n",
    "En este notebook, vamos a explorar las técnicas más comunes que podemos utilizar para mejorar nuestros datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza\n",
    "\n",
    "En ocaciones es conveniente eliminar algunas columnas, o bien cambiar los tipos (int, float, datetime etc). En nuestro dataset de carros, vamos a eliminar el nombre de los carros y vamos a “castear” las variables a un tipo numérico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/auto-mpg.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar una columna (drop)\n",
    "data.drop('car name',axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las columnas son tipo object, vamos a convertir los datos en tipo numerico\n",
    "\n",
    "# convertir columnas en datos numericos\n",
    "for c in data.columns:\n",
    "    data[c] = pd.to_numeric(data[c], errors ='coerce')\n",
    "    \n",
    "# coerce conviete datos invalidos en NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a revisar a ver si existen columnas con valores NaN. Para esto podemos usar Seaborn!\n",
    "#!pip3 install seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(data.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a reemplazar los valores nulos por la mediana. (mediana vs media?)\n",
    "data['horsepower'] = data['horsepower'].fillna(data['horsepower'].median())\n",
    "\n",
    "# vamos a revisar si los valores nulos\n",
    "sns.heatmap(data.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacciones\n",
    "\n",
    "Las interacciones son uniones de 2 o más variables para crear una nueva. Las interacciones se pueden ejecutar con variables numéricas o categóricas.\n",
    "\n",
    "Si encontramos una relacion entre dos variables, podemos combinarlas en una sola variable. Vamos a realizar esto con las columnas \"horsepower\" y \"acceleration\". Podemos realizar tantas interacciones como queramos, pero no todas van a contribuir con la capacidad predictiva del modelo.\n",
    "\n",
    "**Nota** las interacciones pueden ayudar en mejorar la capacidad predictiva, pero disminuyen la interpretabilidad del modelo. Por ejemplo, que significa hp_acc = 1560 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a crear una nueva columna: hp_acc\n",
    "data[\"hp_acc\"] = data[\"horsepower\"] * data[\"acceleration\"]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tambien podemos crear interacciones con variables de texto o categoricas. veamos el ejemplo del dataset de Kaggle de los restaurantes de Chipotle en Estados Unidos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/chipotle_stores.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear una nueva columna donde combinamos el state y location en una sola columna, \n",
    "# y eliminanamos state y location.\n",
    "\n",
    "data[\"st_loc\"] = data[\"state\"] + \"_\" +  data[\"location\"]\n",
    "data[\"st_loc\"] = data[\"st_loc\"].astype('category')\n",
    "\n",
    "# eliminamos las variables individuales\n",
    "data.drop('state',axis=1,inplace=True)\n",
    "data.drop('location',axis=1,inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revismos cuantas categorias hay en st_loc\n",
    "data[\"st_loc\"].cat.categories.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacion\n",
    "\n",
    "Muchos algoritmos de aprendizaje automático son sensibles a las dimensiones, grados de libertad y magnitudes de las columnas de los dataset. Por tanto es importante conocer algunas formas de reducir este impacto mediante técnicas de normalizacion: escalamiento y estandarización. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalas\n",
    "\n",
    "Cuando pensamos en escalas, tratamos de transformar los datos de su estado actual a un rango tal como [0,1] o [-1,1]. Cuando escalamos datos, la distribución de los datos no cambia, solamente cambiamos la proporción. Esta técnica es particularmente útil para algoritmos sensibles a las distancias entre los datos tales como SVM y KNN. \n",
    "\n",
    "A continuación, vamos a explorar las técnicas más populares de “scaling” con Sklearn.\n",
    "\n",
    "Existen otras opciones tambien como:\n",
    "- Student t-statistic\n",
    "- Standarized Moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MinMaxScaler**\n",
    "\n",
    "Transforma los datos en el rango de [0,1]\n",
    "\n",
    "Fórmula:\n",
    "\n",
    "$x_{scaled}=\\frac{x-min(X)}{max(X)-min(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([-10,-5,4,1,0,2,4,6,8,10]).reshape(-1,1)\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "\n",
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **StandardScaler**\n",
    "\n",
    "Esta técnica hace que la media de los datos generados sea 0 con una desviación standard de 0.  Esto causa que el 68% de los datos quede entre -1 y 1.\n",
    "\n",
    "Fórmula:\n",
    "\n",
    "$z=\\frac{x-u}{s}$\n",
    "\n",
    "donde:\n",
    "- u: es la media del training set\n",
    "- s: es la desviacion standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([-10,-5,4,1,0,2,4,6,8,10]).reshape(-1,1)\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RobustScaler**\n",
    "\n",
    "Esta técnica hace uso de estadísticas que son robustas en la presencia de “outliers”. Es similar al MinMaxScaler, solo que en lugar de usar el min y el max, usa el IQR.\n",
    "\n",
    "Este escalador elimina la mediana y escala los datos de acuerdo con el rango de cuantiles (el valor predeterminado es IQR: rango intercuartílico). El IQR es el rango entre el primer cuartil (cuantil 25) y el tercer cuartil (cuantil 75).\n",
    "\n",
    "Fórmula:\n",
    "\n",
    "$x_{scaled}=\\frac{x-Q_1(x)}{Q_3(x)-x-Q_1(x)}$\n",
    "\n",
    "donde:\n",
    "- Q: son los IQRs\n",
    "\n",
    "<a href=\"https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/measuring-spread-quantitative/v/calculating-interquartile-range-iqr\">Que es el IQR?</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([-10,-5,4,1,0,2,4,6,8,10,1000]).reshape(-1,1)\n",
    "scaler = RobustScaler().fit(X)\n",
    "\n",
    "scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estandarización (Transformaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mapeo de datos hacia una distribución Gaussiana (Box-Cox & Yeo-johnson)** \n",
    "\n",
    "Esta es una técnica útil cuando los datos presentan heterocedasticidad.  Algo que pasa con regularidad en modelos de regresión lineal.\n",
    "\n",
    "PowerTransformer permite la transformación Box-Cox y la transformación Yeo-Johnson. Box-Cox requiere que los datos de entrada sean estrictamente positivos, mientras que Yeo-Johnson admite datos tanto positivos como negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "data = pd.read_csv('https://gist.githubusercontent.com/sachinsdate/c2a92fd009c62fee9364c835aff7e2f0/raw/424abc43c42647f5d8831241ee92c7d3093dc610/monthly_gold_price_index_fred.csv')\n",
    "\n",
    "plt.scatter(data.DATE, data.Export_Price_Index_of_Gold)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer # Yeo-johnson por defecto.\n",
    "\n",
    "x = data.Export_Price_Index_of_Gold\n",
    "\n",
    "pt = PowerTransformer().fit(np.array(x).reshape(-1,1))\n",
    "price = pt.transform(np.array(x).reshape(-1,1))\n",
    "\n",
    "plt.scatter(data.DATE, price)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformacion con np.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.Export_Price_Index_of_Gold\n",
    "price = np.array(x).reshape(-1,1)\n",
    "\n",
    "# Log Transform!\n",
    "price = np.log(price)\n",
    "\n",
    "plt.scatter(data.DATE, price)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Normalización L1 y L2**\n",
    "\n",
    "La normalización es el proceso de escalar muestras individuales para tener una norma unitaria. Esto permite calcular la magnitud de los vectores o matrices. \n",
    "\n",
    "- L1: es la suma de los values absolutos del vector. L1(v) = ||v||1\n",
    "- L2: es raiz de la suma de los de los valores del vector al cuadrado. L2(v) = sqrt(a1^2 + ... + ak^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "X_normalized = normalize(X, norm='l2')\n",
    "\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "X_normalized = normalize(X, norm='l1')\n",
    "\n",
    "X_normalized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
