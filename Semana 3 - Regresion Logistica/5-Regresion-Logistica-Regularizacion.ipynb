{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización\n",
    "\n",
    "Que es?\n",
    "- La regularización es cualquier modificación que hacemos a un algoritmo de aprendizaje que tiene como objetivo reducir su error de generalización (osea el del test set) pero no su error de entrenamiento.\n",
    "- Es probable que al aplicar regularización, se incremente el error en el training set, pero también esperamos bajarlo en el test set.\n",
    "- La regularización busca reducir el overfitting, y se espera que el algoritmo se comporte mejor (mejor fit) con datos nunca vistos.\n",
    "- Los dos tipos mas comunes son regularización L1 Laplace y L2 Gaussiana. Ambos métodos buscan reducir el tamano de los coeficientes a través de las interaciones. En el caso de regresión logística, esta penalización se realiza en la función del costo y en el gradiente.\n",
    "\n",
    "### Funcion del Gradiente de Logistic Regression\n",
    "\n",
    "def grads($x, y, b_i, b_0$)\n",
    "- $fval = b_i^TX + b_0$\n",
    "- $ypred = sig(fval)$\n",
    "- $err' = bce'(y,ypred)$\n",
    "- $fval' = sig'(fval)$\n",
    "- $b_i =  \\sum (err' * fval', x)$\n",
    "- $b_0 =  \\sum (err' * fval')$\n",
    "- $devuelve(b_i, b_0)$\n",
    "\n",
    "\n",
    "### Regularización L1 - Lasso\n",
    "\n",
    "def grads($x, y, b_i, b_0$)\n",
    "- $fval = b_i^TX + b_0$\n",
    "- $ypred = sig(fval)$\n",
    "- $err' = bce'(y,ypred) + [\\frac{\\lambda}{2m} * \\sum(|b_i|) + b_0]$\n",
    "- $fval' = sig'(fval)$\n",
    "- $b_i =  \\sum (err' * fval', x) - [\\frac{\\lambda}{m} * b_i]$\n",
    "- $b_0 =  \\sum (err' * fval')$\n",
    "- $devuelve(b_i, b_0)$\n",
    "\n",
    "### Regularización L2 - Ridge\n",
    "\n",
    "def grads($x, y, b_i, b_0$)\n",
    "- $fval = b_i^TX + b_0$\n",
    "- $ypred = sig(fval)$\n",
    "- $err' = bce'(y,ypred) + [\\frac{\\lambda}{2m} * \\sum(b_i^2) + b_0^2]$\n",
    "- $fval' = sig'(fval)$\n",
    "- $b_i =  \\sum (err' * fval', x) - [\\frac{\\lambda}{m} * b_i]$\n",
    "- $b_0 =  \\sum (err' * fval')$\n",
    "- $devuelve(b_i, b_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestras bibliotecas importantes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carga el dataset de los lirios\n",
    "l_petalo = [2,2.5,1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4, 4.7, 4.5, 4.9, 4, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4, 4.9, 4.7, 4.3, 4.4, 4.8, 5, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4, 4.4, 4.6, 4, 3.3, 4.2, 4.2, 4.2, 4.3, 3, 4.1, 6, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5, 5.1, 5.3, 5.5, 6.7, 6.9, 5, 5.7, 4.9, 6.7, 4.9, 5.7, 6, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5, 5.2, 5.4, 5.1]\n",
    "a_petalo = [2,0.5,0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1, 1.3, 1.4, 1, 1.5, 1, 1.4, 1.3, 1.4, 1.5, 1, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1, 1.1, 1, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3, 2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2, 1.9, 2.1, 2, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2, 2, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2, 2.3, 1.8]\n",
    "especie = ['setosa','setosa','setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n",
    "\n",
    "# Creamos una matriz de diseño con las variables independientes\n",
    "datos_x = np.array((l_petalo, a_petalo)).transpose()\n",
    "\n",
    "# seed  = 42\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificamos el vector 'especie' para los tres clasificadores distintos\n",
    "especie_setosa = np.array([1.0 if x=='setosa' else 0.0 for x in especie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función sigmoide y su derivada\n",
    "def sig(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def d_sig(x):\n",
    "    return sig(x) * (1.0 - sig(x))\n",
    "\n",
    "# Función de error y su derivada\n",
    "def bce(y_real, y_pred):\n",
    "    return np.sum(-y_real*np.log(y_pred) - (1.0 - y_real) * np.log(1.0 - y_pred))\n",
    "\n",
    "def d_bce(y_real, y_pred):\n",
    "    return (y_pred - y_real) / (y_pred * (1.0-y_pred))\n",
    "\n",
    "# Cálculo de gradientes\n",
    "def grads(x, y_real, cur_bi, cur_b0):    \n",
    "    f_val = np.dot(x, cur_bi) + cur_b0\n",
    "    y_pred = sig(f_val)\n",
    "    d_err = d_bce(y_real, y_pred)\n",
    "    d_f_val = d_sig(f_val)\n",
    "    d_bi = np.dot(d_err * d_f_val, x)\n",
    "    d_b0 = np.sum(d_err * d_f_val)\n",
    "    return (d_bi, d_b0)\n",
    "\n",
    "# Función de optimización con gradiente descendiente\n",
    "def gd(x, y, lr=0.001, num_iter=1000):\n",
    "    M = x.shape[1]\n",
    "    bi = np.random.randn(M)\n",
    "    [b0] = np.random.randn(1)\n",
    "    errs = []\n",
    "    \n",
    "    for ix in range(num_iter):\n",
    "        pred = sig(np.dot(x, bi) + b0)\n",
    "        err = bce(y, pred)\n",
    "        (d_bi, d_b0) = grads(x, y, bi, b0)\n",
    "        bi = bi - lr * d_bi\n",
    "        b0 = b0 - lr * d_b0\n",
    "        errs.append(err)\n",
    "        \n",
    "    return (errs,bi,b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se entrena el modelo para detectar la clase setosa (unicamente)\n",
    "(errs_setosa, bi_setosa, b0_setosa) = gd(datos_x, especie_setosa)\n",
    "\n",
    "# Graficamos las tres curvas de error a través de las iteraciones\n",
    "plt.plot(errs_setosa)\n",
    "plt.scatter(np.arange(0, 1000), errs_setosa, s=40, c=\"red\", marker='x')\n",
    "print(\"error:\",errs_setosa[len(errs_setosa)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de predicción (probabilidad) para un clasificador por separado\n",
    "def prob_uno(params, l_petalo, a_petalo):\n",
    "    \n",
    "    # saquemos los parámetros\n",
    "    (bi, b0) = params\n",
    "    \n",
    "    # construyamos un vector de valores para predecir\n",
    "    vec = np.array([l_petalo, a_petalo])\n",
    "    \n",
    "    return sig(np.dot(bi, vec) + b0)\n",
    "\n",
    "# Función de predicción (etiqueta) para los tres clasificadores juntos\n",
    "def predict(l_petalo, a_petalo):\n",
    "    \n",
    "    # saquemos los conjuntos de parámetros\n",
    "    params_setosa = (bi_setosa, b0_setosa)\n",
    "    \n",
    "    # evaluamos las tres funciones por separado\n",
    "    prob_setosa = prob_uno(params_setosa, l_petalo, a_petalo)\n",
    "    \n",
    "    return 1.0 if prob_setosa > 0.5 else 0.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prima = np.array([predict(x[0],x[1]) for x in datos_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de coordinadas estimadas de la barrera de desicion.\n",
    "plot_x_n = np.array([min(datos_x[:,0]), max(datos_x[:,0])])\n",
    "plot_y_n = (-1/bi_setosa[0]) * (bi_setosa[1] * plot_x_n + b0_setosa)\n",
    "\n",
    "plt.plot(plot_x_n, plot_y_n, '.--', c = \"red\")\n",
    "plt.scatter(datos_x[:,0],datos_x[:,1], c = especie_setosa, cmap= \"winter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.sum((y_prima == especie_setosa) * 1) / len(especie_setosa)\n",
    "print(\"Exactitud sin Regularizacion:\", acc * 100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizacion L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros de regularizacion\n",
    "lamb = 0.05\n",
    "m = datos_x.shape[0]\n",
    "\n",
    "# Cálculo de gradientes\n",
    "def grads(x, y_real, cur_bi, cur_b0):\n",
    "    \n",
    "    # penalizacion al costo -lambda/2m * sum(betas al cuadrado)\n",
    "    bce_reg = (lamb/(2*m) * (np.sum(cur_bi**2)+cur_b0**2))\n",
    "    \n",
    "    f_val = np.dot(x, cur_bi) + cur_b0\n",
    "    y_pred = sig(f_val)\n",
    "    \n",
    "    # se aplica penalizacion\n",
    "    d_err = d_bce(y_real, y_pred) + bce_reg\n",
    "    d_f_val = d_sig(f_val)\n",
    "    \n",
    "    # penalizacion al gradiente bi\n",
    "    grad_reg = (lamb/m * cur_bi)\n",
    "    \n",
    "    # se aplica penalizacion\n",
    "    d_bi = np.dot(d_err * d_f_val, x) - grad_reg\n",
    "    d_b0 = np.sum(d_err * d_f_val)\n",
    "    return (d_bi, d_b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prima = np.array([predict(x[0],x[1]) for x in datos_x])\n",
    "\n",
    "# se entrena el modelo para detectar la clase setosa (unicamente)\n",
    "(errs_setosa, bi_setosa, b0_setosa) = gd(datos_x, especie_setosa)\n",
    "\n",
    "# Graficamos las tres curvas de error a través de las iteraciones\n",
    "plt.plot(errs_setosa)\n",
    "plt.scatter(np.arange(0, 1000), errs_setosa, s=40, c=\"blue\", marker='x')\n",
    "print(\"error:\",errs_setosa[len(errs_setosa)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de coordinadas estimadas de la barrera de desicion.\n",
    "plot_x_l2 = np.array([min(datos_x[:,0]), max(datos_x[:,0])])\n",
    "plot_y_l2 = (-1/bi_setosa[0]) * (bi_setosa[1] * plot_x_l2 + b0_setosa)\n",
    "\n",
    "plt.plot(plot_x_n, plot_y_n, '.-', c = \"red\", label = \"normal\")\n",
    "plt.plot(plot_x_l2, plot_y_l2, '.--', c = \"black\", label = \"Ridge\") # con regularizacion\n",
    "plt.scatter(datos_x[:,0],datos_x[:,1], c = especie_setosa, cmap= \"winter\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.sum((y_prima == especie_setosa) * 1) / len(especie_setosa)\n",
    "print(\"Exactitud Con Regularizacion L2:\", acc * 100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros de regularizacion\n",
    "lamb = 0.05\n",
    "m = datos_x.shape[0]\n",
    "\n",
    "# Cálculo de gradientes\n",
    "def grads(x, y_real, cur_bi, cur_b0):\n",
    "    \n",
    "    # penalizacion al costo -lambda/2m * sum(betas al cuadrado)\n",
    "    bce_reg = (lamb/(2*m) * (np.sum(cur_bi)+cur_b0))\n",
    "    \n",
    "    f_val = np.dot(x, cur_bi) + cur_b0\n",
    "    y_pred = sig(f_val)\n",
    "    \n",
    "    # se aplica penalizacion\n",
    "    d_err = d_bce(y_real, y_pred) + bce_reg\n",
    "    d_f_val = d_sig(f_val)\n",
    "    \n",
    "    # penalizacion al gradiente bi\n",
    "    grad_reg = (lamb/m * cur_bi)\n",
    "    \n",
    "    # se aplica penalizacion\n",
    "    d_bi = np.dot(d_err * d_f_val, x) - grad_reg\n",
    "    d_b0 = np.sum(d_err * d_f_val)\n",
    "    return (d_bi, d_b0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prima = np.array([predict(x[0],x[1]) for x in datos_x])\n",
    "\n",
    "# se entrena el modelo para detectar la clase setosa (unicamente)\n",
    "(errs_setosa, bi_setosa, b0_setosa) = gd(datos_x, especie_setosa)\n",
    "\n",
    "# Graficamos las tres curvas de error a través de las iteraciones\n",
    "plt.plot(errs_setosa)\n",
    "plt.scatter(np.arange(0, 1000), errs_setosa, s=40, c=\"purple\", marker='x')\n",
    "print(\"error:\",errs_setosa[len(errs_setosa)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de coordinadas estimadas de la barrera de desicion.\n",
    "plot_x_l1 = np.array([min(datos_x[:,0]), max(datos_x[:,0])])\n",
    "plot_y_l1 = (-1/bi_setosa[0]) * (bi_setosa[1] * plot_x_l1 + b0_setosa)\n",
    "\n",
    "plt.plot(plot_x_n, plot_y_n, '.-', c = \"red\", label = \"normal\")\n",
    "plt.plot(plot_x_l2, plot_y_l2, '.--', c = \"black\", label = \"Ridge\") # con regularizacion\n",
    "plt.plot(plot_x_l1, plot_y_l1, '.--', c = \"purple\", label = \"Lasso\") # con regularizacion\n",
    "plt.scatter(datos_x[:,0],datos_x[:,1], c = especie_setosa, cmap= \"winter\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
