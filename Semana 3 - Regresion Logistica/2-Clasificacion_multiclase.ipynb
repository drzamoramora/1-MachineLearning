{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación multiclase\n",
    "\n",
    "Ya hemos construido nuestro primer modelo de clasificación binario. Comenzamos por una sola variable independiente, introducimos varias, y ahora haremos una extensión importante: **clasificación con más de dos categorías**. Un ejemplo de esto podría ser el aprender una función que, con las coordenadas de latitud y longitud de un punto geográfico dentro del territorio nacional, nos devuelve una de las siguientes:\n",
    "\n",
    "- Una etiqueta $\\texttt{provincia} \\in \\left\\{\\texttt{San Jose}, \\texttt{Alajuela}, \\texttt{Cartago}, \\texttt{Heredia}, \\texttt{Guanacaste}, \\texttt{Puntarenas}, \\texttt{Limon} \\right\\}$. O, en su defecto,\n",
    "- Un vector $\\mathbf{y}$ con siete valores que denotan la probabilidad de que el punto indicado se encuentre en una provincia dada, de modo que $\\sum_{i=1}^{7} y_i = 1$.\n",
    "\n",
    "\n",
    "### El dataset de los lirios\n",
    "\n",
    "![image](img/lirios.png)\n",
    "\n",
    "<center>Imagen tomada de <a href=\"https://www.datacamp.com/community/tutorials/machine-learning-in-r/\">Machine Learning in R for beginners</a></center>\n",
    "\n",
    "Para este notebook, haremos uso de uno de los datasets más famosos y antiguos de la historia de la estadística: los **datos de los lirios de Fisher**, conocido en inglés como el *Iris dataset*. Este dataset contiene muestras de mediciones de lirios, con cinco columnas: **la longitud y ancho del pétalo y del sépalo, respectivamente**, y una etiqueta que indica la especie de lirio al que corresponde, que puede ser *versicolor*, *virginica* o *setosa*. Nuestro objetivo, entonces, es aprender una de las dos funciones, de acuerdo con las descripciones que dimos arriba \n",
    "\n",
    "- $f\\left(\\texttt{l}\\_\\texttt{petalo}, \\texttt{a}\\_\\texttt{petalo}, \\texttt{l}\\_\\texttt{sepalo}, \\texttt{a}\\_\\texttt{sepalo} \\right) \\mapsto \\left\\{\\texttt{versicolor}, \\texttt{virginica}, \\texttt{setosa}  \\right\\}$. Es decir, nuestra función retorna una **etiqueta específica** sin mención a su probabilidad—sin embargo, inferimos que es más probable que las otras dos\n",
    "\n",
    "- $f\\left(\\texttt{l}\\_\\texttt{petalo}, \\texttt{a}\\_\\texttt{petalo}, \\texttt{l}\\_\\texttt{sepalo}, \\texttt{a}\\_\\texttt{sepalo} \\right) \\mapsto \\left[ p\\left(\\texttt{versicolor}\\right), p\\left(\\texttt{virginica} \\right), p\\left(\\texttt{setosa} \\right)\\right]$. Es decir, un **vector de probabilidades**, de modo que $p\\left(\\texttt{versicolor}\\right) + p\\left(\\texttt{virginica} \\right) + p\\left(\\texttt{setosa} \\right) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestras bibliotecas importantes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carga el dataset de los lirios\n",
    "l_petalo = [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1.5, 1.7, 1.5, 1, 1.7, 1.9, 1.6, 1.6, 1.5, 1.4, 1.6, 1.6, 1.5, 1.5, 1.4, 1.5, 1.2, 1.3, 1.4, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6, 1.9, 1.4, 1.6, 1.4, 1.5, 1.4, 4.7, 4.5, 4.9, 4, 4.6, 4.5, 4.7, 3.3, 4.6, 3.9, 3.5, 4.2, 4, 4.7, 3.6, 4.4, 4.5, 4.1, 4.5, 3.9, 4.8, 4, 4.9, 4.7, 4.3, 4.4, 4.8, 5, 4.5, 3.5, 3.8, 3.7, 3.9, 5.1, 4.5, 4.5, 4.7, 4.4, 4.1, 4, 4.4, 4.6, 4, 3.3, 4.2, 4.2, 4.2, 4.3, 3, 4.1, 6, 5.1, 5.9, 5.6, 5.8, 6.6, 4.5, 6.3, 5.8, 6.1, 5.1, 5.3, 5.5, 5, 5.1, 5.3, 5.5, 6.7, 6.9, 5, 5.7, 4.9, 6.7, 4.9, 5.7, 6, 4.8, 4.9, 5.6, 5.8, 6.1, 6.4, 5.6, 5.1, 5.6, 6.1, 5.6, 5.5, 4.8, 5.4, 5.6, 5.1, 5.1, 5.9, 5.7, 5.2, 5, 5.2, 5.4, 5.1]\n",
    "a_petalo = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0.3, 0.2, 0.4, 0.2, 0.5, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.4, 0.1, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.2, 0.3, 0.3, 0.2, 0.6, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 1.4, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6, 1, 1.3, 1.4, 1, 1.5, 1, 1.4, 1.3, 1.4, 1.5, 1, 1.5, 1.1, 1.8, 1.3, 1.5, 1.2, 1.3, 1.4, 1.4, 1.7, 1.5, 1, 1.1, 1, 1.2, 1.6, 1.5, 1.6, 1.5, 1.3, 1.3, 1.3, 1.2, 1.4, 1.2, 1, 1.3, 1.2, 1.3, 1.3, 1.1, 1.3, 2.5, 1.9, 2.1, 1.8, 2.2, 2.1, 1.7, 1.8, 1.8, 2.5, 2, 1.9, 2.1, 2, 2.4, 2.3, 1.8, 2.2, 2.3, 1.5, 2.3, 2, 2, 1.8, 2.1, 1.8, 1.8, 1.8, 2.1, 1.6, 1.9, 2, 2.2, 1.5, 1.4, 2.3, 2.4, 1.8, 1.8, 2.1, 2.4, 2.3, 1.9, 2.3, 2.5, 2.3, 1.9, 2, 2.3, 1.8]\n",
    "l_sepalo = [5.1, 4.9, 4.7, 4.6, 5, 5.4, 4.6, 5, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5, 5, 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5, 5.5, 4.9, 4.4, 5.1, 5, 4.5, 4.4, 5, 5.1, 4.8, 5.1, 4.6, 5.3, 5, 7, 6.4, 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5, 5.9, 6, 6.1, 5.6, 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7, 6, 5.7, 5.5, 5.5, 5.8, 6, 5.4, 6, 6.7, 6.3, 5.6, 5.5, 5.5, 6.1, 5.8, 5, 5.6, 5.7, 5.7, 6.2, 5.1, 5.7, 6.3, 5.8, 7.1, 6.3, 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5, 7.7, 7.7, 6, 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2, 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6, 6.9, 6.7, 6.9, 5.8, 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9]\n",
    "a_sepalo = [3.5, 3, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3, 3, 4, 4.4, 3.9, 3.5, 3.8, 3.8, 3.4, 3.7, 3.6, 3.3, 3.4, 3, 3.4, 3.5, 3.4, 3.2, 3.1, 3.4, 4.1, 4.2, 3.1, 3.2, 3.5, 3.6, 3, 3.4, 3.5, 2.3, 3.2, 3.5, 3.8, 3, 3.8, 3.2, 3.7, 3.3, 3.2, 3.2, 3.1, 2.3, 2.8, 2.8, 3.3, 2.4, 2.9, 2.7, 2, 3, 2.2, 2.9, 2.9, 3.1, 3, 2.7, 2.2, 2.5, 3.2, 2.8, 2.5, 2.8, 2.9, 3, 2.8, 3, 2.9, 2.6, 2.4, 2.4, 2.7, 2.7, 3, 3.4, 3.1, 2.3, 3, 2.5, 2.6, 3, 2.6, 2.3, 2.7, 3, 2.9, 2.9, 2.5, 2.8, 3.3, 2.7, 3, 2.9, 3, 3, 2.5, 2.9, 2.5, 3.6, 3.2, 2.7, 3, 2.5, 2.8, 3.2, 3, 3.8, 2.6, 2.2, 3.2, 2.8, 2.8, 2.7, 3.3, 3.2, 2.8, 3, 2.8, 3, 2.8, 3.8, 2.8, 2.8, 2.6, 3, 3.4, 3.1, 3, 3.1, 3.1, 3.1, 2.7, 3.2, 3.3, 3, 2.5, 3, 3.4, 3]\n",
    "especie = ['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica']\n",
    "\n",
    "# Creamos una matriz de diseño con las variables independientes\n",
    "datos_x = np.array((l_petalo, a_petalo, l_sepalo, a_sepalo)).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación uno-contra-todos\n",
    "\n",
    "La primera técnica de clasificación multiclase que veremos es la denominada **uno-contra-todos** (en inglés *one-vs-all*). Esta es una **técnica general** que puede ser aplicada a **cualquier modelo de clasificación binario** y no solo a la regresión logística. De hecho, esta técnica es de uso común en modelos que no tienen una generalización multiclase natural, como las **máquinas de soporte vectorial** (SVMs). \n",
    "\n",
    "La técnica es simple y consiste en entrenar **un modelo de clasificación binaria separado para cada etiqueta**. Por ejemplo, en nuestro ejemplo de las provincias, tendríamos que entrenar **siete** modelos distintos. En el caso de los lirios, entrenaremos **tres modelos distintos**, cada uno de la forma $$f\\left(\\texttt{l}\\_\\texttt{petalo}, \\texttt{a}\\_\\texttt{petalo}, \\texttt{l}\\_\\texttt{sepalo}, \\texttt{a}\\_\\texttt{sepalo} \\right) \\mapsto [0,1]$$ de modo que, por ejemplo, el modelo de ``versicolor`` es un modelo que nos devuelve la probabilidad de que el lirio sea de especie *versicolor*, y de igual manera con ``virginica`` y ``setosa``.\n",
    "\n",
    "Para poder hacer esto, a la hora de entrenar los tres modelos, tenemos que crear un **vector de entrenamiento distinto para cada uno**, en el que la variable dependiente está codificada de manera binaria de acuerdo a la etiqueta correspondiente. Por ejemplo, el modelo que entrenaremos para ``setosa`` tendrá un vector ``especie_setosa`` codificado de modo que ``1=setosa`` y ``0=todo-lo-que-no-es-setosa``. Lo haremos de modo similar con los otros dos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificamos el vector 'especie' para los tres clasificadores distintos\n",
    "especie_setosa = np.array([1.0 if x=='setosa' else 0.0 for x in especie])\n",
    "especie_virginica = np.array([1.0 if x=='virginica' else 0.0 for x in especie])\n",
    "especie_versicolor = np.array([1.0 if x=='versicolor' else 0.0 for x in especie])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, procederemos a generar nuestros tres modelos de regresión logística. Pero antes, retomemos nuestro código de gradiente descendiente del notebook previo, con leves modificaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función sigmoide y su derivada\n",
    "def sig(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def d_sig(x):\n",
    "    return sig(x) * (1.0 - sig(x))\n",
    "\n",
    "# Función de error y su derivada\n",
    "def bce(y_real, y_pred):\n",
    "    return np.sum(-y_real*np.log(y_pred) - (1.0 - y_real)*np.log(1.0 - y_pred))\n",
    "\n",
    "def d_bce(y_real, y_pred):\n",
    "    return (y_pred - y_real) / (y_pred * (1.0-y_pred))\n",
    "\n",
    "# Cálculo de gradientes\n",
    "def grads(x, y_real, cur_alpha, cur_beta):    \n",
    "    f_val = np.dot(x, cur_alpha) + cur_beta\n",
    "    y_pred = sig(f_val)\n",
    "    d_err = d_bce(y_real, y_pred)\n",
    "    d_f_val = d_sig(f_val)\n",
    "    d_alpha = np.dot(d_err * d_f_val, x)\n",
    "    d_beta = np.sum(d_err * d_f_val)\n",
    "    return (d_alpha, d_beta)\n",
    "\n",
    "# Función de optimización con gradiente descendiente\n",
    "def gd(x, y, lr=0.001, num_iter=1000):\n",
    "    M = x.shape[1]\n",
    "    alpha = np.random.randn(M)\n",
    "    [beta] = np.random.randn(1)\n",
    "    errs = []\n",
    "    \n",
    "    for ix in range(num_iter):\n",
    "        pred = sig(np.dot(x, alpha) + beta)\n",
    "        err = bce(y, pred)\n",
    "        (d_alpha, d_beta) = grads(x, y, alpha, beta)\n",
    "        alpha = alpha - lr*d_alpha\n",
    "        beta = beta - lr*d_beta\n",
    "        errs.append(err)\n",
    "        \n",
    "    return (errs,alpha,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Entrenemos los tres modelos\n",
    "(errs_setosa, alpha_setosa, beta_setosa) = gd(datos_x, especie_setosa)\n",
    "(errs_virginica, alpha_virginica, beta_virginica) = gd(datos_x, especie_virginica)\n",
    "(errs_versicolor, alpha_versicolor, beta_versicolor) = gd(datos_x, especie_versicolor)\n",
    "\n",
    "# Graficamos las tres curvas de error a través de las iteraciones\n",
    "plt.plot(errs_setosa)\n",
    "plt.plot(errs_virginica)\n",
    "plt.plot(errs_versicolor)\n",
    "\n",
    "# Coloquemos los parámetros de cada modelo en tuplas (para facilitar el código más abajo)\n",
    "params_setosa = (alpha_setosa, beta_setosa)\n",
    "params_virginica = (alpha_virginica, beta_virginica)\n",
    "params_versicolor = (alpha_versicolor, beta_versicolor)\n",
    "\n",
    "# Coloquemos todos los parámetros en una lista de tuplas\n",
    "params_all = [params_setosa, params_virginica, params_versicolor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinando todos en uno\n",
    "Ahora que tenemos los tres modelos, ¿cómo los unimos para producir nuestra función de predicción multiclase? El procedimiento es simple. Dados los valores ``l_petalo``, ``a_petalo``, ``l_sepalo``, ``a_sepalo``, haremos lo siguiente:\n",
    "\n",
    "1. Ejecutamos cada uno de los tres modelos por separado, obteniendo tres valores $p\\left(\\texttt{versicolor} \\right)$, $p\\left(\\texttt{setosa} \\right)$ y $p\\left(\\texttt{virginica} \\right)$\n",
    "\n",
    "2. Encontramos el valor más alto de los tres y retornamos la etiqueta que le corresponde.\n",
    "\n",
    "Eso es todo. Hagámoslo en código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de predicción (probabilidad) para un clasificador por separado\n",
    "def prob_uno(params, l_petalo, a_petalo, l_sepalo, a_sepalo):\n",
    "    \n",
    "    # saquemos los parámetros\n",
    "    (alpha, beta) = params\n",
    "    \n",
    "    # construyamos un vector de valores para predecir\n",
    "    vec = np.array([l_petalo, a_petalo, l_sepalo, a_sepalo])\n",
    "    \n",
    "    return np.dot(alpha, vec) + beta\n",
    "\n",
    "# Función de predicción (etiqueta) para los tres clasificadores juntos\n",
    "def predict(params, l_petalo, a_petalo, l_sepalo, a_sepalo):\n",
    "    \n",
    "    # saquemos los conjuntos de parámetros\n",
    "    [params_setosa, params_virginica, params_versicolor] = params\n",
    "    \n",
    "    # evaluamos las tres funciones por separado\n",
    "    prob_setosa = prob_uno(params_setosa, l_petalo, a_petalo, l_sepalo, a_sepalo)\n",
    "    prob_virginica = prob_uno(params_virginica, l_petalo, a_petalo, l_sepalo, a_sepalo)\n",
    "    prob_versicolor = prob_uno(params_versicolor, l_petalo, a_petalo, l_sepalo, a_sepalo)\n",
    "    \n",
    "    # encontremos la que tiene la mayor probabilidad\n",
    "    which_max = np.argmax([prob_setosa, prob_virginica, prob_versicolor])\n",
    "    \n",
    "    # y retornamos la etiqueta correspondiente\n",
    "    labels = [\"setosa\", \"virginica\", \"versicolor\"]    \n",
    "    return labels[which_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 / 150 valores correctos\n"
     ]
    }
   ],
   "source": [
    "# Probemos nuestro esquema contra el mismo dataset\n",
    "pred = []\n",
    "puntos = 0\n",
    "for i in range(len(especie)):\n",
    "    px = predict(params_all, l_petalo[i], a_petalo[i], l_sepalo[i], a_sepalo[i])\n",
    "    pred.append(px)\n",
    "    \n",
    "    if (px == especie[i]):\n",
    "        puntos += 1\n",
    "\n",
    "print(puntos, '/', len(especie), \"valores correctos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué hay de malo con uno-contra-todos? \n",
    "\n",
    "A pesar de ser un método popular, generalmente efectivo y fácil de implementar, uno-contra-todos tiene varias desventajas:\n",
    "\n",
    "1. El entrenar varios modelos implica correr **varias optimizaciones separadas**. Esto puede convertirse en un **problema de desempeño** si el número de etiquetas y el tamaño del dataset son ambos grandes. \n",
    "\n",
    "2. Las probabilidades de los distintos modelos pueden no encontrarse en la misma escala. \n",
    "\n",
    "3. Class imbalance.\n",
    "\n",
    "4. Very small / very big probabilities.\n",
    "\n",
    "5. Dado que las probabilidades de los distintos modelos son independientes, no hay una manera natural de combinarlas en un vector.\n",
    "\n",
    "\n",
    "## Clasificación Softmax\n",
    "\n",
    "Ahora que hemos entendido las limitaciones del método uno-contra-todos, revisaremos una generalización de nuestro modelo de regresión logística"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
